{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSknRz7ufOy4FvC8Mg9fVr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQ8TmXBn2Z_H","executionInfo":{"status":"ok","timestamp":1744341421793,"user_tz":-330,"elapsed":2715,"user":{"displayName":"Aatif Shaikh","userId":"01168834222412149176"}},"outputId":"123db6ca-ad13-4475-909e-12de6a4226a4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yc7az3uV109W","executionInfo":{"status":"ok","timestamp":1744341664427,"user_tz":-330,"elapsed":3845,"user":{"displayName":"Aatif Shaikh","userId":"01168834222412149176"}},"outputId":"956d0264-9562-4310-81d5-160eef9385f8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original Tokens:\n","['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'an', 'exciting', 'field', 'that', 'explores', 'the', 'intersection', 'of', 'linguistics', 'and', 'computer', 'science', '.', 'Through', 'techniques', 'such', 'as', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', ',', 'machines', 'can', 'understand', 'human', 'language', '.']\n","\n","POS Tags:\n","[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('an', 'DT'), ('exciting', 'JJ'), ('field', 'NN'), ('that', 'WDT'), ('explores', 'VBZ'), ('the', 'DT'), ('intersection', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), ('and', 'CC'), ('computer', 'NN'), ('science', 'NN'), ('.', '.'), ('Through', 'IN'), ('techniques', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), (',', ','), ('machines', 'NNS'), ('can', 'MD'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n","\n","Tokens after Stop Words Removal:\n","['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'exciting', 'field', 'explores', 'intersection', 'linguistics', 'computer', 'science', '.', 'techniques', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'machines', 'understand', 'human', 'language', '.']\n","\n","Tokens after Stemming:\n","['natur', 'languag', 'process', '(', 'nlp', ')', 'excit', 'field', 'explor', 'intersect', 'linguist', 'comput', 'scienc', '.', 'techniqu', 'token', ',', 'stem', ',', 'lemmat', ',', 'machin', 'understand', 'human', 'languag', '.']\n","\n","Tokens after Lemmatization:\n","['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'excite', 'field', 'explores', 'intersection', 'linguistics', 'computer', 'science', '.', 'technique', 'tokenization', ',', 'stem', ',', 'lemmatization', ',', 'machine', 'understand', 'human', 'language', '.']\n"]}],"source":["import nltk\n","import re\n","import numpy as np\n","import pandas as pd\n","\n","# Download necessary NLTK data files (only needs to run once)\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","# Define a function to map POS tags from nltk's format to WordNet format\n","def get_wordnet_pos(nltk_pos_tag):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    if nltk_pos_tag.startswith('J'):\n","        return 'a'  # adjective\n","    elif nltk_pos_tag.startswith('V'):\n","        return 'v'  # verb\n","    elif nltk_pos_tag.startswith('N'):\n","        return 'n'  # noun\n","    elif nltk_pos_tag.startswith('R'):\n","        return 'r'  # adverb\n","    else:\n","        return None\n","\n","def preprocess_document(text):\n","    \"\"\"\n","    This function takes a text string and performs:\n","     - Tokenization\n","     - POS Tagging\n","     - Stop words removal\n","     - Stemming\n","     - Lemmatization\n","\n","    It returns a dictionary containing the results after each processing step.\n","    \"\"\"\n","    results = {}\n","\n","    # --- Tokenization ---\n","    tokens = word_tokenize(text)\n","    results['tokens'] = tokens\n","\n","    # --- POS Tagging ---\n","    pos_tags = pos_tag(tokens)\n","    results['pos_tags'] = pos_tags\n","\n","    # --- Stop Words Removal ---\n","    stop_words = set(stopwords.words('english'))\n","    tokens_no_stop = [token for token in tokens if token.lower() not in stop_words]\n","    results['tokens_no_stop'] = tokens_no_stop\n","\n","    # --- Stemming ---\n","    stemmer = PorterStemmer()\n","    tokens_stemmed = [stemmer.stem(token) for token in tokens_no_stop]\n","    results['tokens_stemmed'] = tokens_stemmed\n","\n","    # --- Lemmatization ---\n","    lemmatizer = WordNetLemmatizer()\n","    # For lemmatization we use the POS tags of the tokens (from the stop-words removed list)\n","    # We perform POS tagging again on tokens_no_stop to get more accurate mapping.\n","    pos_tags_no_stop = pos_tag(tokens_no_stop)\n","    tokens_lemmatized = []\n","    for token, tag in pos_tags_no_stop:\n","        wn_tag = get_wordnet_pos(tag)\n","        if wn_tag is None:\n","            tokens_lemmatized.append(lemmatizer.lemmatize(token))\n","        else:\n","            tokens_lemmatized.append(lemmatizer.lemmatize(token, pos=wn_tag))\n","    results['tokens_lemmatized'] = tokens_lemmatized\n","\n","    return results\n","\n","# Sample document for preprocessing\n","sample_doc = (\n","    \"Natural Language Processing (NLP) is an exciting field that explores the intersection \"\n","    \"of linguistics and computer science. Through techniques such as tokenization, stemming, \"\n","    \"and lemmatization, machines can understand human language.\"\n",")\n","\n","# Preprocess the sample document\n","processed = preprocess_document(sample_doc)\n","\n","# Print results of each step\n","print(\"Original Tokens:\")\n","print(processed['tokens'])\n","print(\"\\nPOS Tags:\")\n","print(processed['pos_tags'])\n","print(\"\\nTokens after Stop Words Removal:\")\n","print(processed['tokens_no_stop'])\n","print(\"\\nTokens after Stemming:\")\n","print(processed['tokens_stemmed'])\n","print(\"\\nTokens after Lemmatization:\")\n","print(processed['tokens_lemmatized'])"]},{"cell_type":"code","source":["# For TF-IDF, we need a corpus (a collection of documents).\n","# Here we create a small sample corpus.\n","corpus = [\n","    \"Natural language processing is an interdisciplinary field that deals with many aspects of language and computer science.\",\n","    \"Machine learning techniques have dramatically improved the field of natural language processing.\",\n","    \"In this class, we explore the foundations of NLP along with the principles of data science.\"\n","]\n","\n","def clean_document_for_tfidf(text):\n","    processed_text = preprocess_document(text)\n","    # Join the lemmatized tokens into a single string\n","    return \" \".join(processed_text['tokens_lemmatized'])\n","\n","clean_corpus = [clean_document_for_tfidf(doc) for doc in corpus]\n","\n","# Now, we use scikit-learn's TfidfVectorizer to create a TF-IDF representation of the documents.\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(clean_corpus)\n","\n","# Display the feature names (i.e., terms) and the TF-IDF matrix\n","print(\"\\nTF-IDF Feature Names:\")\n","print(vectorizer.get_feature_names_out())\n","print(\"\\nTF-IDF Matrix:\")\n","print(tfidf_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S22ZjSoW1_8R","executionInfo":{"status":"ok","timestamp":1744341729857,"user_tz":-330,"elapsed":13,"user":{"displayName":"Aatif Shaikh","userId":"01168834222412149176"}},"outputId":"fb24cadd-d982-4e9c-cb86-226719d825cc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","TF-IDF Feature Names:\n","['along' 'aspect' 'class' 'computer' 'data' 'deal' 'dramatically'\n"," 'explore' 'field' 'foundation' 'improve' 'interdisciplinary' 'language'\n"," 'learn' 'machine' 'many' 'natural' 'nlp' 'principle' 'processing'\n"," 'science' 'technique']\n","\n","TF-IDF Matrix:\n","[[0.         0.32229243 0.         0.32229243 0.         0.32229243\n","  0.         0.         0.2451117  0.         0.         0.32229243\n","  0.4902234  0.         0.         0.32229243 0.2451117  0.\n","  0.         0.2451117  0.2451117  0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.36977238 0.         0.28122142 0.         0.36977238 0.\n","  0.28122142 0.36977238 0.36977238 0.         0.28122142 0.\n","  0.         0.28122142 0.         0.36977238]\n"," [0.36325471 0.         0.36325471 0.         0.36325471 0.\n","  0.         0.36325471 0.         0.36325471 0.         0.\n","  0.         0.         0.         0.         0.         0.36325471\n","  0.36325471 0.         0.27626457 0.        ]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zrf7D00f1__X"},"execution_count":null,"outputs":[]}]}
